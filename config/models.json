{
  "models": {
    "mistral:7b": {
      "temperature": 0.7,
      "max_tokens": 2048,
      "top_p": 0.9,
      "system_prompt": "You are a helpful AI assistant using the Mistral 7B model. Answer questions accurately and concisely.",
      "hardware_requirements": {
        "min_ram_gb": 8,
        "recommended_ram_gb": 16,
        "gpu_recommended": false
      }
    },
    "deepseek-r1:32b": {
      "temperature": 0.5,
      "max_tokens": 4096,
      "top_p": 0.95,
      "system_prompt": "You are a powerful AI assistant running on the DeepSeek-R1 32B model. Provide detailed, accurate, and well-reasoned responses.",
      "hardware_requirements": {
        "min_ram_gb": 16,
        "recommended_ram_gb": 32,
        "gpu_recommended": true
      }
    },
    "llama3:8b": {
      "temperature": 0.7,
      "max_tokens": 2048,
      "top_p": 0.9,
      "system_prompt": "You are a helpful AI assistant using the Llama 3 8B model. Provide clear and accurate answers.",
      "hardware_requirements": {
        "min_ram_gb": 8,
        "recommended_ram_gb": 16,
        "gpu_recommended": false
      }
    },
    "codellama:13b": {
      "temperature": 0.2,
      "max_tokens": 8192,
      "top_p": 0.95,
      "system_prompt": "You are a helpful AI coding assistant. Write clean, correct, and efficient code with appropriate comments. Explain your code when asked.",
      "hardware_requirements": {
        "min_ram_gb": 16,
        "recommended_ram_gb": 24,
        "gpu_recommended": true
      }
    }
  },
  "task_types": {
    "question_answering": {
      "temperature_adjustment": 0.0,
      "max_tokens_multiplier": 1.0,
      "recommended_models": ["mistral:7b", "deepseek-r1:32b", "llama3:8b"]
    },
    "code_generation": {
      "temperature_adjustment": -0.3,
      "max_tokens_multiplier": 1.5,
      "recommended_models": ["codellama:13b", "deepseek-r1:32b"]
    },
    "planning": {
      "temperature_adjustment": 0.1,
      "max_tokens_multiplier": 1.2,
      "recommended_models": ["deepseek-r1:32b", "mistral:7b"]
    },
    "tool_execution": {
      "temperature_adjustment": -0.1,
      "max_tokens_multiplier": 0.8,
      "recommended_models": ["mistral:7b", "llama3:8b"]
    }
  },
  "default_settings": {
    "default_model": "mistral:7b",
    "mcp_url": "http://localhost:8080",
    "ollama_url": "http://localhost:11434",
    "max_history_items": 10,
    "max_retries": 3,
    "retry_wait_multiplier": 2,
    "retry_max_wait": 30
  }
}
